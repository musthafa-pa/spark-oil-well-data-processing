from pyspark.sql import SparkSessionfrom pyspark.sql.functions import colfrom pyspark.ml.classification import LogisticRegressionfrom pyspark.ml.feature import VectorAssemblerfrom pyspark.ml import Pipeline# Initialize Spark sessionspark = SparkSession.builder.appName("FaultPrevention").getOrCreate()# Sample datadata = [    (1, 75.0, 101.0, 0),    (2, 80.0, 102.0, 0),    (3, 85.0, 100.0, 0),    (4, 90.0, 105.0, 1),    (5, 95.0, 110.0, 1),    (6, 100.0, 120.0, 1),]# Create DataFramecolumns = ["id", "temperature", "pressure", "fault"]df = spark.createDataFrame(data, schema=columns)# Show DataFramedf.show()# Prepare featuresassembler = VectorAssembler(inputCols=["temperature", "pressure"], outputCol="features")data_prepared = assembler.transform(df)# Split data into training and testing sets(trainingData, testData) = data_prepared.randomSplit([0.7, 0.3])# Initialize and train the logistic regression modellr = LogisticRegression(featuresCol="features", labelCol="fault")model = lr.fit(trainingData)# Evaluate the modelpredictions = model.transform(testData)predictions.select("id", "temperature", "pressure", "probability", "prediction", "fault").show()# New sample datanew_data = [    (7, 85.0, 103.0),    (8, 92.0, 108.0),]# Create DataFrame for new datanew_columns = ["id", "temperature", "pressure"]new_df = spark.createDataFrame(new_data, schema=new_columns)# Prepare new datanew_data_prepared = assembler.transform(new_df)# Make predictions on new datanew_predictions = model.transform(new_data_prepared)new_predictions.select("id", "temperature", "pressure", "probability", "prediction").show()